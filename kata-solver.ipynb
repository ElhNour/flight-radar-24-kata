{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Radar 24 Kata Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlightRadar24.api import FlightRadar24API\n",
    "fr_api = FlightRadar24API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting flights list \n",
    "def get_flights_dataframe(fr_api,spark):\n",
    "    flights = fr_api.get_flights()\n",
    "    # Transform to a DataFrame\n",
    "    flights_to_dict=[]\n",
    "    for flight in flights:\n",
    "        flights_to_dict.append({\n",
    "            \"id\":flight.id,\n",
    "            \"icao_24bit\":flight.icao_24bit,\n",
    "            \"latitude\":flight.latitude,\n",
    "            \"longitude\":flight.longitude,\n",
    "            \"heading\":flight.heading,\n",
    "            \"altitude\":flight.altitude,\n",
    "            \"ground_speed\":flight.ground_speed,\n",
    "            \"aircraft_code\":flight.aircraft_code,\n",
    "            \"registration\":flight.registration,\n",
    "            \"time\":flight.time,\n",
    "            \"origin_airport_iata\":flight.origin_airport_iata,\n",
    "            \"destination_airport_iata\":flight.destination_airport_iata,\n",
    "            \"number\":flight.number,\n",
    "            \"airline_iata\":flight.airline_iata,\n",
    "            \"on_ground\":flight.on_ground,\n",
    "            \"vertical_speed\":flight.vertical_speed,\n",
    "            \"callsign\":flight.callsign,\n",
    "            \"airline_icao\":flight.airline_icao\n",
    "        })\n",
    "    flights_df=spark.createDataFrame(data=flights_to_dict).na.replace([\"N/A\",\"NaN\",\"\"],None).dropna()\n",
    "    return flights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting airlines list\n",
    "def get_airlines_dataframe(fr_api,spark):\n",
    "    airlines = fr_api.get_airlines()\n",
    "    # Transform to a DataFrame\n",
    "    airlines_df=spark.createDataFrame(data=airlines).na.replace([\"N/A\",\"NaN\",\"\"],None).dropna()    \n",
    "    return airlines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting zones list\n",
    "zones=fr_api.get_zones()\n",
    "zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_csv(dataframe):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    output_path = f\"Flights/rawzone/tech_year={timestamp[:4]}/tech_month={timestamp[:6]}/tech_day={timestamp[:8]}/flights{timestamp}\"\n",
    "    dataframe.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Companie avec le plus de vols en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_1(flights_df):\n",
    "    grouped_active_flights = flights_df.filter(\"on_ground==0\").groupBy(\"airline_icao\").agg({\"id\": \"count\"}).withColumnRenamed(\"count(id)\", \"count\")\n",
    "    max_count = grouped_active_flights.agg({\"count\": \"max\"}).collect()[0][0]\n",
    "    most_active_airline=grouped_active_flights.filter(F.col(\"count\") == max_count)\n",
    "\n",
    "    # Store results in a csv file\n",
    "    dataframe_to_csv(most_active_airline)\n",
    "\n",
    "    return most_active_airline.collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pour chaque continent, la compagnie avec le plus de vols régionaux actifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def task_2(flights_df,udf_get_continent):\n",
    "    grouped_regional_flights = flights_df.filter(\"origin_airport_iata==destination_airport_iata\" and \"on_ground==0\")\\\n",
    "    .withColumn(\"continent\", udf_get_continent(F.col(\"latitude\"), F.col(\"longitude\")))\\\n",
    "    .groupBy([\"continent\",\"airline_icao\"]).agg(F.count(\"id\")).withColumnRenamed(\"count(id)\", \"count\")\n",
    "\n",
    "    windows=Window.partitionBy(\"continent\").orderBy(F.col(\"count\").desc())\n",
    "\n",
    "    grouped_regional_flights=grouped_regional_flights.withColumn(\"row_number\", F.row_number().over(windows)).filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "\n",
    "    # Store results in a csv file\n",
    "    dataframe_to_csv(grouped_regional_flights)\n",
    "\n",
    "    return grouped_regional_flights.collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Le vol en cours avec le trajet le plus long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_3(flights_df,spark):    \n",
    "    longest_flight_trip= flights_df.orderBy(F.col(\"time\").desc()).first()\n",
    "    df=spark.createDataFrame(data=[longest_flight_trip])\n",
    "    dataframe_to_csv(df)\n",
    "    return longest_flight_trip\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pour chaque continent, la longueur de vol moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_4(flights_df,udf_get_continent):\n",
    "    average_flight_duration_per_continent = flights_df.withColumn(\"continent\", udf_get_continent(F.col(\"latitude\"), F.col(\"longitude\")))\\\n",
    "    .groupBy(\"continent\").avg(\"time\").withColumnRenamed(\"avg(time)\", \"average_time\")\n",
    "\n",
    "    # Store results in a csv file\n",
    "    dataframe_to_csv(average_flight_duration_per_continent)\n",
    "\n",
    "    return average_flight_duration_per_continent.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. L'entreprise constructeur d'avions avec le plus de vols actifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def task_5(flights_df,spark):    \n",
    "    most_active_aircraft = flights_df.filter(\"on_ground==0\").groupBy(\"aircraft_code\").agg({\"id\": \"count\"}).withColumnRenamed(\"count(id)\", \"count\")\\\n",
    "        .orderBy(F.col(\"count\").desc()).first()\n",
    "    df=spark.createDataFrame(data=[most_active_aircraft])\n",
    "    dataframe_to_csv(df)\n",
    "    return most_active_aircraft\n",
    "# join with planes table to get the manifucturer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pour chaque pays de compagnie aérienne, le top 3 des modèles d'avion en usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_6(flights_df):    \n",
    "    top_3_aircrafts_per_country = flights_df.groupBy([\"airline_iata\",\"aircraft_code\"]).agg({\"id\": \"count\"}).withColumnRenamed(\"count(id)\", \"count\") \\\n",
    "        .orderBy(F.col(\"count\").desc())\n",
    "\n",
    "    windows=Window.partitionBy(\"airline_iata\").orderBy(F.col(\"count\").desc())\n",
    "\n",
    "    top_3_aircrafts_per_country=top_3_aircrafts_per_country.withColumn(\"row_number\", F.row_number().over(windows)).filter(F.col(\"row_number\") <= 3).drop(\"row_number\")\n",
    "\n",
    "\n",
    "    # Store results in a csv file\n",
    "    dataframe_to_csv(top_3_aircrafts_per_country)\n",
    "\n",
    "    return top_3_aircrafts_per_country.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continent(latitude,longitude,zones):\n",
    "    for continent, value in zones.items():\n",
    "        if value[\"tl_x\"] <= latitude <= value[\"br_x\"] and  value[\"br_y\"]<= longitude <=value[\"tl_y\"] :\n",
    "            return continent\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FlightRadar24ETL():\n",
    "    fr_api = FlightRadar24API()\n",
    "    spark=SparkSession.builder.getOrCreate()\n",
    "    flights_df=get_flights_dataframe(fr_api,spark)\n",
    "    # print(flights_df.head(3))\n",
    "    zones=fr_api.get_zones()\n",
    "    udf_get_continent=F.udf(get_continent)\n",
    "    \n",
    "\n",
    "    task_1(flights_df)\n",
    "    task_2(flights_df,udf_get_continent)\n",
    "    task_3(flights_df,spark)\n",
    "    task_4(flights_df,udf_get_continent)\n",
    "    task_5(flights_df,spark)\n",
    "    task_6(flights_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nour/miniconda3/envs/lsdm/lib/python3.10/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/home/nour/miniconda3/envs/lsdm/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/nour/miniconda3/envs/lsdm/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "WARNING:apscheduler.scheduler:Execution of job \"FlightRadar24ETL (trigger: interval[0:00:05], next run at: 2023-05-30 20:49:30 CEST)\" skipped: maximum number of running instances reached (1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Exécuter le pipeline toutes les 2 heures\u001b[39;00m\n\u001b[1;32m      6\u001b[0m scheduler\u001b[39m.\u001b[39madd_job(FlightRadar24ETL, \u001b[39m'\u001b[39m\u001b[39minterval\u001b[39m\u001b[39m'\u001b[39m, seconds\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m scheduler\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/miniconda3/envs/lsdm/lib/python3.10/site-packages/apscheduler/schedulers/blocking.py:21\u001b[0m, in \u001b[0;36mBlockingScheduler.start\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event \u001b[39m=\u001b[39m Event()\n\u001b[1;32m     20\u001b[0m \u001b[39msuper\u001b[39m(BlockingScheduler, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mstart(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_main_loop()\n",
      "File \u001b[0;32m~/miniconda3/envs/lsdm/lib/python3.10/site-packages/apscheduler/schedulers/blocking.py:30\u001b[0m, in \u001b[0;36mBlockingScheduler._main_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m wait_seconds \u001b[39m=\u001b[39m TIMEOUT_MAX\n\u001b[1;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m!=\u001b[39m STATE_STOPPED:\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(wait_seconds)\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event\u001b[39m.\u001b[39mclear()\n\u001b[1;32m     32\u001b[0m     wait_seconds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_jobs()\n",
      "File \u001b[0;32m~/miniconda3/envs/lsdm/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/lsdm/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "\n",
    "scheduler = BlockingScheduler()\n",
    "\n",
    "# Exécuter le pipeline toutes les 2 heures\n",
    "scheduler.add_job(FlightRadar24ETL, 'interval', seconds=5)\n",
    "\n",
    "scheduler.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
